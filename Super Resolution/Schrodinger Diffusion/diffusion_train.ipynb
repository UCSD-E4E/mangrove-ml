{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Metal Performance Shaders (MPS) device.\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to be used for training and evaluation\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=DEVICE)\n",
    "    print(\"Using CUDA device.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=DEVICE)\n",
    "    print(\"Using Apple Metal Performance Shaders (MPS) device.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"No GPU found. Defaulting to CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import JupyterArgParser\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= global settings =========\n",
    "# Taken from i2sb paper with minor changes\n",
    "\n",
    "RESULT_DIR = Path(\"results\")\n",
    "\n",
    "# --------------- basic ---------------\n",
    "parser = JupyterArgParser()\n",
    "parser.add_argument(\"--seed\",           type=int,   default=0)\n",
    "parser.add_argument(\"--name\",           type=str,   default=None,        help=\"experiment ID\")\n",
    "parser.add_argument(\"--ckpt\",           type=str,   default=None,        help=\"resumed checkpoint name\")\n",
    "parser.add_argument(\"--device\",         type=str,   default=DEVICE,      help=\"type of device to use for training\")\n",
    "parser.add_argument(\"--gpu\",            type=int,   default=None,        help=\"set only if you wish to run on a particular GPU\")\n",
    "\n",
    "# --------------- model ---------------\n",
    "parser.add_argument(\"--image-size\",     type=int,   default=256)\n",
    "parser.add_argument(\"--t0\",             type=float, default=1e-4,        help=\"sigma start time in network parametrization\")\n",
    "parser.add_argument(\"--T\",              type=float, default=1.,          help=\"sigma end time in network parametrization\")\n",
    "parser.add_argument(\"--interval\",       type=int,   default=1000,        help=\"number of interval\")\n",
    "parser.add_argument(\"--beta-max\",       type=float, default=0.3,         help=\"max diffusion for the diffusion model\")\n",
    "parser.add_argument(\"--beta-schedule\",  type=str,   default=\"i2sb\",    help=\"schedule for beta\")\n",
    "parser.add_argument(\"--ot-ode\",         action=\"store_true\",             help=\"use OT-ODE model\")\n",
    "parser.add_argument(\"--clip-denoise\",   action=\"store_true\",             help=\"clamp predicted image to [-1,1] at each\")\n",
    "parser.add_argument(\"--use-fp16\",       action=\"store_true\",             help=\"use fp16 for training\")\n",
    "parser.add_argument(\"diffusion-type\",   type=str,   default=\"schrodinger_bridge\",      help=\"type of diffusion model\")\n",
    "\n",
    "# --------------- optimizer and loss ---------------\n",
    "parser.add_argument(\"--batch-size\",     type=int,   default=256)\n",
    "parser.add_argument(\"--microbatch\",     type=int,   default=2,           help=\"accumulate gradient over microbatch until full batch-size\")\n",
    "parser.add_argument(\"--num-itr\",        type=int,   default=1000000,     help=\"training iteration\")\n",
    "parser.add_argument(\"--lr\",             type=float, default=5e-5,        help=\"learning rate\")\n",
    "parser.add_argument(\"--lr-gamma\",       type=float, default=0.99,        help=\"learning rate decay ratio\")\n",
    "parser.add_argument(\"--lr-step\",        type=int,   default=1000,        help=\"learning rate decay step size\")\n",
    "parser.add_argument(\"--l2-norm\",        type=float, default=0.0)\n",
    "parser.add_argument(\"--ema\",            type=float, default=0.99)\n",
    "\n",
    "# --------------- path and logging ---------------\n",
    "parser.add_argument(\"--dataset-dir\",    type=Path,  default=\"/dataset\",  help=\"path to LMDB dataset\")\n",
    "parser.add_argument(\"--log-dir\",        type=Path,  default=\".log\",      help=\"path to log std outputs and writer data\")\n",
    "parser.add_argument(\"--log-writer\",     type=str,   default=None,        help=\"log writer: can be tensorbard, wandb, or None\")\n",
    "parser.add_argument(\"--wandb-api-key\",  type=str,   default=None,        help=\"unique API key of your W&B account; see https://wandb.ai/authorize\")\n",
    "parser.add_argument(\"--wandb-user\",     type=str,   default=None,        help=\"user name of your W&B account\")\n",
    "parser.add_argument(\"--ckpt-path\",      type=Path,  default=None,        help=\"path to save checkpoints\")\n",
    "parser.add_argument(\"--load\",           type=Path,  default=None,        help=\"path to load checkpoints\")\n",
    "parser.add_argument(\"--unet_path\",           type=str,   default=None,        help=\"path of UNet model to load for training\")\n",
    "\n",
    "# --------------- distributed ---------------\n",
    "parser.add_argument(\"--local-rank\",     type=int,   default=0)\n",
    "parser.add_argument(\"--global-rank\",    type=int,   default=0)\n",
    "parser.add_argument(\"--global-size\",    type=int,   default=1)\n",
    "\n",
    "opt = parser.get_options()\n",
    "\n",
    "# ========= path handle =========\n",
    "opt.name = \"test\"\n",
    "os.makedirs(opt.log_dir, exist_ok=True)\n",
    "opt.ckpt_path = RESULT_DIR / opt.name if opt.name else RESULT_DIR / \"temp\"\n",
    "os.makedirs(opt.ckpt_path, exist_ok=True)\n",
    "\n",
    "if opt.ckpt:\n",
    "    ckpt_file = RESULT_DIR / opt.ckpt / \"latest.pt\"\n",
    "    assert ckpt_file.exists()\n",
    "    opt.load = ckpt_file\n",
    "else:\n",
    "    opt.load = None\n",
    "\n",
    "# ========= auto assert =========\n",
    "assert opt.batch_size % opt.microbatch == 0, f\"{opt.batch_size=} is not dividable by {opt.microbatch}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import SuperResolutionDataset\n",
    "\n",
    "# build dataset      \n",
    "hr_images = [torch.randn(1, 3, opt.image_size, opt.image_size)]\n",
    "lr_images = [torch.randn(1, 3, opt.image_size, opt.image_size)]\n",
    "assert len(hr_images) == len(lr_images)\n",
    "\n",
    "images = len(hr_images)\n",
    "split = int(0.8*images)\n",
    "train_hr, val_hr = hr_images[:split], hr_images[split:]\n",
    "train_lr, val_lr = lr_images[:split], lr_images[split:]\n",
    "\n",
    "train = SuperResolutionDataset(hr_images=train_hr, lr_images=train_lr, transform=None)\n",
    "val = SuperResolutionDataset(hr_images=val_hr, lr_images=val_lr, transform=None)\n",
    "\n",
    "del train_hr, val_hr, train_lr, val_lr, hr_images, lr_images\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gage/anaconda3/envs/i2sb/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built schrodinger_bridge Diffusion Model with 1000 steps and i2sb beta schedule!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m run \u001b[38;5;241m=\u001b[39m Runner(opt)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Diffusion/i2sb/runner.py:151\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self, opt, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    148\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_inner_loop):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# sample from boundary pair\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     x0, x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     step \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, opt\u001b[38;5;241m.\u001b[39minterval, (x0\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],))\n\u001b[1;32m    153\u001b[0m     xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mq_sample(step, x0, x1, ot_ode\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mot_ode)\n",
      "File \u001b[0;32m~/Desktop/Diffusion/i2sb/runner.py:276\u001b[0m, in \u001b[0;36mRunner.sample_batch\u001b[0;34m(self, opt, loader, scale_factor)\u001b[0m\n\u001b[1;32m    274\u001b[0m         low_res_img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(low_res_img, high_res_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[0;32m--> 276\u001b[0m     clean_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    278\u001b[0m         high_res_img \u001b[38;5;241m=\u001b[39m clean_img\u001b[38;5;241m.\u001b[39mto(opt\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Desktop/Diffusion/i2sb/util.py:21\u001b[0m, in \u001b[0;36msetup_loader\u001b[0;34m(dataset, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_loader\u001b[39m(dataset, batch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoaderX\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/i2sb/lib/python3.8/site-packages/torch/utils/data/dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/i2sb/lib/python3.8/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from i2sb import Runner\n",
    "\n",
    "# build runner\n",
    "run = Runner(opt)\n",
    "# train\n",
    "run.train(opt, train, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transition plotter\n",
    "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "\n",
    "torch.manual_seed(opt.seed)\n",
    "def plot_images(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    _, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mangrove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
