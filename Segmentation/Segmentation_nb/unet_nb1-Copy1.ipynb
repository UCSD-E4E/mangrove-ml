{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load \"../ml-mangrove/Segmentation/unet.pyp\" \n",
    "import segmentation_models as sm\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import rasterio\n",
    "import subprocess\n",
    "import tensorflow_datasets as tfds\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from segmentation_models.utils import set_trainable\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from gis_utils import raster\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing notebooks\n",
    "#from ipynb.fs.full.<notebook_name> import *\n",
    "from ipynb.fs.full.create_seg_dataset import create_seg_dataset\n",
    "from ipynb.fs.full.gen_seg_labels import gen_seg_labels, tif_to_jpg, tile_raster\n",
    "from ipynb.fs.full.raster_mask import raster_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources: https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/\n",
    "\n",
    "'''\n",
    "Documentation/Usage: This script is meant to be called with command line arguments.\n",
    "--width (required): tile width\n",
    "--input_rasters (required): space separated list of rasters (orthomosaics)\n",
    "--input_vectors (required for training): space separated list of shapefiles (ordering should correspond with rasters)\n",
    "--train: Flag. Add if training.\n",
    "--test: Flag. Add if testing.\n",
    "--weights (required): path to weights file, either to write to for training, or to use for testing (.h5)\n",
    "--backbone (required): name of backbone to use, ex: resnet34, vgg16\n",
    "\n",
    "For training it should be sufficient to just call the script using the list of rasters and vectors (and other required arguments), \n",
    "and currently you have to manually set the hyperparams in the code, but this should eventually be offloaded to a settings file or \n",
    "command line arguments. This will result in the training weights being saved in the specified .h5 file.\n",
    "\n",
    "For testing you just need to call the script on the list of rasters and it will produce a mask of the entire\n",
    "orthomosaic.\n",
    "'''\n",
    "#keras.backend.set_image_data_format('channels_first')\n",
    "sm.set_framework('tf.keras')    # need this otherwise currently a bug in model.fit when used with tf.Datasets\n",
    "\n",
    "# Globals\n",
    "N_CHANNELS = 3\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "LOSS_FUNC = sm.losses.DiceLoss()\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(img_path: str) -> dict:\n",
    "    \"\"\"Load an image and its annotation (mask) and returning\n",
    "    a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : str\n",
    "        Image (not the mask) location.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # Creating mask path from image path\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"images\", \"annotations\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"image\", \"annotation\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    # The masks contain a class index for each pixels\n",
    "    mask = tf.image.decode_jpeg(mask, channels=1)\n",
    "    mask = tf.image.convert_image_dtype(mask, tf.uint8)\n",
    "    \n",
    "    #mask = tf.where(mask == 255, np.dtype('uint8').type(0), mask)\n",
    "    # Note that we have to convert the new value (0)\n",
    "    # With the same dtype than the tensor itself\n",
    "    \n",
    "\n",
    "    return {'image': image, 'segmentation_mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images/masks between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_mask = tf.cast(input_mask, tf.float32) / 255.0 # attempting to fix metrics    \n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "    \"\"\"Apply some transformations to an input dictionary\n",
    "    containing a train image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An annotation is a regular  channel image.\n",
    "    If a transformation such as rotation is applied to the image,\n",
    "    the same transformation has to be applied on the annotation also.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "   \n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    #input_mask = tf.image.rgb_to_grayscale(datapoint['segmentation_mask'])\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "    \n",
    "    #input_mask = tf.reshape(input_mask, (HEIGHT, WIDTH))  # removing single channel\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_val(datapoint: dict) -> tuple:\n",
    "    \"\"\"Normalize and resize a test image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since this is for the val set, we don't need to apply\n",
    "    any data augmentation technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    \n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    #input_mask = tf.reshape(input_mask, (HEIGHT, WIDTH)) # removing single channel\n",
    "\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image(datapoint: dict) -> tuple:\n",
    "    \"\"\"Loads and image and resizes it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    #input_mask = tf.image.resize(datapoint['label'], (HEIGHT, WIDTH))\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmin(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    #return pred_mask[0]\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model=None, dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(backbone, weight_file):\n",
    "    # For tensorboard\n",
    "    logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch')\n",
    "\n",
    "\n",
    "    # For more information about autotune:\n",
    "    # https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Data\n",
    "    training_data = \"../dataset/training/\"\n",
    "    #val_data = \"../dataset/validation/\"\n",
    "\n",
    "    # Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    # Creating and splitting dataset\n",
    "    DATASET_SIZE = len(glob(training_data + \"images/*.jpg\"))\n",
    "    print(f\"The Training Dataset contains {DATASET_SIZE} images.\")\n",
    "\n",
    "    TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "    VAL_SIZE = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "    full_dataset = tf.data.Dataset.list_files(training_data + \"images/*.jpg\", seed=SEED)\n",
    "    full_dataset = full_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "    val_dataset = full_dataset.skip(TRAIN_SIZE)\n",
    "    \n",
    "    # Creating dict pairs linking images and annotations\n",
    "    train_dataset = train_dataset.map(parse_image)\n",
    "    val_dataset = val_dataset.map(parse_image)\n",
    "\n",
    "    # -- Train Dataset --# - https://stackoverflow.com/questions/49915925/output-differences-when-changing-order-of-batch-shuffle-and-repeat\n",
    "    train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    train_dataset = train_dataset.repeat()\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    #-- Validation Dataset --#\n",
    "    #for image, label in tfds.as_numpy(val_dataset):\n",
    "     # print(type(image), type(label), label)\n",
    "    \n",
    "    val_dataset = val_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "    val_dataset = val_dataset.repeat()\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    # for multi gpu distributed processing\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # define model within scope to enable distributed learning\n",
    "    with strategy.scope():\n",
    "        model = sm.Unet(\n",
    "        backbone,\n",
    "        weights=weight_file,\n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=NUM_CLASSES,\n",
    "        activation='sigmoid'\n",
    "        )\n",
    "        \n",
    "        # adding l1 regularization\n",
    "        l1_reg = keras.regularizers.l1(0.1)\n",
    "        sm.utils.set_regularization(model, kernel_regularizer=l1_reg)\n",
    "        \n",
    "        model.compile(\n",
    "            'Adam', \n",
    "            loss=LOSS_FUNC, \n",
    "            metrics=[sm.metrics.iou_score] #was giving score over 100 in later epochs before normalizing masks\n",
    "            #[tf.keras.metrics.MeanIoU(num_classes=2)]]\n",
    "        )\n",
    "        \n",
    "    #Saves model with lowest loss on validation set\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(weight_file, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=True, \n",
    "                             mode='auto', save_frequency='epoch')\n",
    "    \n",
    "    #Stops Model training if loss on validation set does notdecrease after 30 epochs\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                            monitor='val_loss', min_delta=0, patience=40, verbose=0, mode='auto',\n",
    "                            baseline=None, restore_best_weights=True\n",
    ")\n",
    "    # TODO research step sizes\n",
    "    history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=350,\n",
    "    initial_epoch=229, # resuming training 8/18/2020\n",
    "    steps_per_epoch=TRAIN_SIZE / BATCH_SIZE,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps= 0.2 * (VAL_SIZE / BATCH_SIZE),\n",
    "    callbacks=[tensorboard_callback, checkpoint, early_stop]\n",
    "    )\n",
    "\n",
    "    # Saving model\n",
    "    #model.save_weights(\"unet_500_weights_vgg16.h5\")\n",
    "    #model.save_weights(weight_file)\n",
    "\n",
    "    # For reinstantiation\n",
    "    #model = keras.models.load_model(your_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display slice of batch in datasets, as images\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def showDataImgs(train_dataset, val_dataset, n):\n",
    "    train_dataset_np = tfds.as_numpy(train_dataset)\n",
    "    val_dataset_np = tfds.as_numpy(val_dataset)\n",
    "    \n",
    "\n",
    "    for i in range(n):\n",
    "        example = next(val_dataset_np)\n",
    "    image = example['image']\n",
    "    label = example['segmentation_mask']\n",
    "    \n",
    "    #print(\"val_label: \", label.shape)#label[11])\n",
    "    #print(\"val_image: \", image.shape)\n",
    "    \n",
    "    #label_slice = label[n].reshape(HEIGHT,WIDTH)\n",
    "    #label_slice = np.around(label_slice)\n",
    "    #plt.imshow(label_slice, cmap='gray')\n",
    "    label = label.reshape(HEIGHT,WIDTH)\n",
    "    plt.imshow(label)\n",
    "    plt.show()\n",
    "    plt.imshow(image)\n",
    "    #plt.imshow(image[n])\n",
    "    plt.show()\n",
    "    unique = np.unique(label)\n",
    "    print(unique)\n",
    "    if len(unique) > 2:\n",
    "        with np.printoptions(threshold=np.inf):\n",
    "            print(label)\n",
    "    #print(label_slice)\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        example = next(train_dataset_np)\n",
    "    image = example['image']\n",
    "    label = example['segmentation_mask']\n",
    "    \n",
    "    #print(\"train_label: \",label.shape)#label[15])\n",
    "    #print(\"train_image: \",image.shape)\n",
    "    \n",
    "    #label_slice = label[n].reshape(HEIGHT,WIDTH)\n",
    "    #label_slice = np.around(label_slice)\n",
    "    #plt.imshow(label_slice, cmap='gray')\n",
    "    label = label.reshape(HEIGHT,WIDTH)\n",
    "    plt.imshow(label)\n",
    "    plt.show()\n",
    "    plt.imshow(image)\n",
    "    #plt.imshow(image[n])\n",
    "    plt.show()\n",
    "    unique = np.unique(label)\n",
    "    print(unique)\n",
    "    if len(unique) > 2:\n",
    "        with np.printoptions(threshold=np.inf):\n",
    "            print(label)\n",
    "    #print(label_slice)\n",
    "\n",
    "    #print(image[0])\n",
    "    # label_grey = np.mean(label[n], -1)\n",
    "    # label_grey.reshape((256,256, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use to view images/masks in dataset\n",
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# SEED = 42\n",
    "# training_data = \"../dataset/training/\"\n",
    "# BATCH_SIZE = 16\n",
    "# BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "# # Creating and splitting dataset\n",
    "# DATASET_SIZE = len(glob(training_data + \"images/*.jpg\"))\n",
    "# print(f\"The Training Dataset contains {DATASET_SIZE} images.\")\n",
    "\n",
    "# TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "# VAL_SIZE = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "# full_dataset = tf.data.Dataset.list_files(training_data + \"images/*.jpg\", seed=SEED)\n",
    "# full_dataset = full_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "# train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "# val_dataset = full_dataset.skip(TRAIN_SIZE)\n",
    "    \n",
    "# # Creating d1ict pairs linking images and annotations\n",
    "# train_dataset = train_dataset.map(parse_image)\n",
    "# val_dataset = val_dataset.map(parse_image)\n",
    "\n",
    "# showDataImgs(train_dataset, val_dataset, 3)\n",
    "\n",
    "# # -- Train Dataset --# - https://stackoverflow.com/questions/49915925/output-differences-when-changing-order-of-batch-shuffle-and-repeat\n",
    "# train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "# train_dataset = train_dataset.repeat()\n",
    "# train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "# train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# #-- Validation Dataset --#\n",
    "# val_dataset = val_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "# val_dataset = val_dataset.repeat()\n",
    "# val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "# val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(\"../dataset/training/vectors/masks/mask_binary.png\")\n",
    "# data = np.asarray(image)\n",
    "# print(type(data))\n",
    "# print(data.shape)\n",
    "# print(np.unique(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example in train_dataset.take(1):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "#for example in next(val_dataset_np):\n",
    "  #print(example)\n",
    "  #image = example[0]\n",
    "  #label = example[1]\n",
    "  #label = tf.cast(label, tf.float32) / 255.0 #normalizing label  \n",
    "  #print((image==label).all())\n",
    "  #print(image.size)\n",
    "#  image_img = Image.fromarray(image, 'RGB')\n",
    " #\n",
    "  #plt.imshow(image)\n",
    "  #plt.show()\n",
    "#   image_img.save('image_test.png')\n",
    "#   image_img.show()\n",
    "\n",
    "#  label_img = Image.fromarray(label, 'RGB')\n",
    "  #label= np.reshape(label, (256,256))\n",
    "  #plt.imshow(label)\n",
    "  #plt.show()\n",
    "#   label_img.save('label_test.png')\n",
    "#   label_img.show()\n",
    "\n",
    "# for example in val_dataset.take(2):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "#   #print(example)\n",
    "#   #image = example[0]\n",
    "#   #label = example[1]\n",
    "#   print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(backbone, weight_file, vector_files, raster_files):\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Relevant directories/files\n",
    "    image_dir = \"../dataset/testing/images\"\n",
    "    annotation_dir = \"../dataset/testing/annotations\"\n",
    "    out_dir = \"../dataset/testing/output\"\n",
    "    testing_data = \"../dataset/testing/\"\n",
    "    model_weights = weight_file\n",
    "\n",
    "    #Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "       try:\n",
    "           for gpu in gpus:\n",
    "               tf.config.experimental.set_memory_growth(gpu, True)\n",
    "           logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "           print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "       except RuntimeError as e:\n",
    "           print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    model = sm.Unet(\n",
    "        #'vgg16', \n",
    "        backbone,\n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        weights=model_weights,\n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes= NUM_CLASSES, \n",
    "        activation='sigmoid'\n",
    "    )\n",
    "\n",
    "    # Might be unnecessary\n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        #loss=sm.losses.bce_jaccard_loss, \n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=[sm.metrics.iou_score]\n",
    "    )\n",
    "    \n",
    "    test_dataset = glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "\n",
    "    #Loop for inference\n",
    "    print(\"\\nStarting inference... \\n\")\n",
    "    for img_file in tqdm(test_dataset):\n",
    "        tif_file = img_file.replace(\"jpg\", \"tif\")\n",
    "\n",
    "        img = np.asarray(Image.open(img_file)) / 255.0 # normalization (not) needed as we dont normalize the img for training\n",
    "        img = img[np.newaxis, ...] # needs (batch_size, height, width, channels)\n",
    "        pred_mask = model.predict(img)[0]\n",
    "        pred_mask = create_mask(pred_mask)\n",
    "        pred_mask = np.array(pred_mask).astype('uint8') * 255\n",
    "        #print(pred_mask)\n",
    "        # Reading metadata from .tif\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            tif_meta = src.meta\n",
    "            tif_meta['count'] = 1\n",
    "\n",
    "        # Writing prediction mask as a .tif using extracted metadata\n",
    "        mask_file = tif_file.replace(\"images\", \"output\")\n",
    "        \n",
    "        with rasterio.open(mask_file, \"w\", **tif_meta) as dest:\n",
    "            # Rasterio needs [bands, width, height]\n",
    "            pred_mask = np.rollaxis(pred_mask, axis=2)\n",
    "            dest.write(pred_mask)\n",
    "\t#printing out metrics\n",
    "\t#results = model.evaluate(img, pred_mask, batch_size=128)\n",
    "\t#print(\"IOU: \", results) \n",
    "    print(\"Merging tiles (to create mask ortho)...\")\n",
    "    call = \"gdal_merge.py -o \" + testing_data + \"ortho_mask.tif \" + \" \" + out_dir + \"/*\"\n",
    "    print(call)\n",
    "    subprocess.call(call, shell=True)\n",
    "    \n",
    "#     print(\"Creating raster_masks...\")\n",
    "#     vector_file = vector_files[0]\n",
    "#     raster_file = raster_files[0]\n",
    "#     raster_mask(raster_files[0], vector_files[0])\n",
    "#     temp_dir = os.path.dirname(vector_file)\n",
    "#     mask_file = os.path.join(temp_dir, \"masks\", \"mask_binary.tif\")\n",
    "    \n",
    "#     #out_dir = os.path.dirname(raster_file)\n",
    "#     gen_seg_labels(out_width, raster_file, vector_file, mask_file, image_dir, True, True)\n",
    "    \n",
    "    \n",
    "#     test_dataset = tf.data.Dataset.list_files(testing_data + \"images/*.jpg\", seed=SEED)\n",
    "#     test_dataset = test_dataset.map(parse_image)\n",
    "#     test_dataset = test_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "#     test_dataset = test_dataset.batch(16)\n",
    "    \n",
    "#     map_file = os.path.join(image_dir, \"map.txt\")\n",
    "#     create_seg_dataset(map_file, \"testing\", 0)\n",
    "#     logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch')\n",
    "#     print(\"Evaluating...\")\n",
    "#     model.evaluate(test_dataset, \n",
    "#                   callbacks=tensorboard_callback\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimized(backbone, weight_file):\n",
    "    '''\n",
    "    Note: This version of test does not work yet. It is optimized to be very efficient and works well for inference on .jpg files.\n",
    "    It lacks the capabilities to link the output predictions to the input .jpgs since the filenames are lost when in the tf.dataset\n",
    "    we map the parse image function. As a result, we need to somehow modify this dataset to retain filename information so we can use it\n",
    "    to link the output prediction to the input image and its corresponding .tif file, which will be used to write the geospatial info to\n",
    "    the prediction.\n",
    "\n",
    "    Initial ideas would be to modify the parse image function and related functions to save filename info, and use this to link the images\n",
    "    in the prediction stage by replacing .jpg with .tif in the filename.\n",
    "    '''\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Relevant directories/files\n",
    "    images = \"../dataset/testing/images\"\n",
    "    annotations = \"../dataset/testing/annotations\"\n",
    "    testing_data = \"../dataset/testing/\"\n",
    "    #model_weights = \"unet_500_weights_vgg16.h5\"\n",
    "    model_weights = weight_file\n",
    "\n",
    "    # Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    model = sm.Unet(\n",
    "        'resnet34', \n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        weights=model_weights,\n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=2, \n",
    "        activation='softmax'\n",
    "    )\n",
    "\n",
    "    # Might be unnecessary\n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        loss=LOSS_FUNC, \n",
    "        metrics=[sm.metrics.iou_score]\n",
    "    )\n",
    "\n",
    "    test_dataset = tf.data.Dataset.list_files(testing_data + \"images/*.jpg\", seed=SEED)\n",
    "    test_dataset = test_dataset.map(parse_image)\n",
    "    test_dataset = test_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    image_data = []\n",
    "    annotation_data = []\n",
    "    \n",
    "    '''\n",
    "    for img_file in tqdm(os.listdir(images)): \n",
    "        annotation_file = \"annotation_\" + img_file.split('_')[1]\n",
    "        img_file = os.path.join(images, img_file)\n",
    "        ann_file = os.path.join(annotations, annotation_file)\n",
    "        image = np.array(Image.open(img_file))\n",
    "        annotation = np.array(Image.open(ann_file))\n",
    "        image_data.append(image)\n",
    "        annotation_data.append(annotation)\n",
    "    '''\n",
    "\n",
    "\n",
    "    #prediction = model.predict(test_dataset, steps=1)\n",
    "    #print(type(prediction))\n",
    "\n",
    "\n",
    "    #display([first_image[0], first_mask[0], create_mask(first_pred_mask)])\n",
    "\n",
    "    #pred_mask = model.predict(test_dataset)\n",
    "    #display([image[0], mask[0], create_mask(pred_mask)])\n",
    "\n",
    "    show_predictions(model=model, dataset=test_dataset, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #view downsmapled testing output ortho_mask.tif\n",
    "# # !sudo apt-get update\n",
    "# # !sudo apt-get install libgdal-dev -y\n",
    "# # !sudo apt-get install python-gdal -y\n",
    "# # !sudo apt-get install python-numpy python-scipy -y\n",
    "# # !pip install rasterio\n",
    "# # !pip install fiona\n",
    "# # !pip install geopandas\n",
    "# # !pip install -i https://test.pypi.org/simple/ gis-utils-pkg-dillhicks==0.0.1\n",
    "# from gis_utils import raster\n",
    "# from rasterio.plot import show\n",
    "\n",
    "# #img_1, meta2 = raster.load_image(\"../\")\n",
    "# img_10, meta1 = raster.load_image(\"../dataset/training/vectors/masks/mask_binary.png\")\n",
    "# #img_1, meta10 = raster.load_image(\"../dataset/training/vectors/masks/mask.tif\")\n",
    "\n",
    "# #downsampling images \n",
    "# ds_factor = 1\n",
    "# #resampled_1, transform = raster.downsample_raster(img_1, ds_factor)\n",
    "# #resampled_10, transform = raster.downsample_raster(img_10, ds_factor)\n",
    "# show(img_10)\n",
    "\n",
    "# #for f in os.listdir(\"../dataset/training/annotations\"):\n",
    "#     #image = image.imRead(\"../dataset/training/annotations/\" + f)\n",
    "#     #show(image)\n",
    "#     #pyplot.imshow(image)\n",
    "#     #pyplot.show()\n",
    "#     #print(np.asarray(image))\n",
    "#     #img, meta = raster.load_image(\"../dataset/training/annotations/\" + f)\n",
    "#     #show(img)\n",
    "#     #print(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_setup(raster_files, vector_files, out_width):\n",
    "    # Uses raster and vector file to create dataset for training\n",
    "    data_files = zip(raster_files, vector_files)\n",
    "    map_files = [] \n",
    "    folderpath = \"../dataset/training/\"\n",
    "    for raster_file, vector_file in data_files:\n",
    "        # Generates raster masks\n",
    "        print(\"Creating raster_masks...\")\n",
    "        raster_mask(raster_file, vector_file)\n",
    "        temp_dir = os.path.dirname(vector_file)\n",
    "        mask_file = os.path.join(temp_dir, \"masks\", \"mask_binary.tif\")\n",
    "\n",
    "        # Generates segmentation labels\n",
    "        out_dir = os.path.dirname(raster_file)\n",
    "        gen_seg_labels(out_width, raster_file, vector_file, mask_file, out_dir, True, True)\n",
    "        map_file = os.path.join(out_dir, \"map.txt\")\n",
    "        map_files.append(map_file)\n",
    "        \n",
    "        #show downsampled raster_mask\n",
    "        img_1, meta1 = raster.load_image(\"../dataset/training/vectors/masks/mask.tif\")\n",
    "        resampled_1, transform = raster.downsample_raster(img_1, 1/5)\n",
    "        show(resampled_1)\n",
    "        \n",
    "        shutil.rmtree(folderpath + \"vectors/masks\")\n",
    "        shutil.rmtree(folderpath + \"vectors/nm\")  #removng directories\n",
    "        shutil.rmtree(folderpath + \"vectors/m\")\n",
    "\n",
    "    # Creating dataset to train UNet\n",
    "    create_seg_dataset(map_files, \"training\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_setup(raster_files, out_width):\n",
    "    out_dir = \"../dataset/testing/output\"\n",
    "    test_dir = \"../dataset/testing\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    print(\"\\nTiling rasters...\")\n",
    "    for raster_file in raster_files:\n",
    "        tile_raster(out_width, raster_file, test_dir, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize arguments\n",
    "img_p = \"../dataset/training/images/\"\n",
    "vec_p = \"../dataset/training/vectors/\"\n",
    "\n",
    "weight_file = \"../dataset/training/weights/08_12_vgg16_350_full_weight.h5\"\n",
    "backbone = \"vgg16\"\n",
    "out_width = \"256\"\n",
    "\n",
    "#raster_files = [\"../dataset/training/images/lap_2018-07_site04_120m_RGB_cc.tif\", \"../dataset/training/images/lap_2019-07_site06_120m_RGB_quick.tif\"]\n",
    "#vector_files = [\"../dataset/training/vectors/lap_2018-07_site04_labels_m-nm.shp\"]#, \"../dataset/training/vectors/lap_2019-07_site06_labels_m-nm.shp\"]\n",
    "raster_files = [\n",
    "                img_p + \"lap_2018-07_site1_120m_RGB_cc.tif\", \n",
    "                img_p + \"lap_2019-07_site03_120m_RGB_quick.tif\",\n",
    "                img_p + \"lap_2018-07_site05_120m_RGB_cc.tif\",\n",
    "                img_p + \"lap_2019-07_site06_120m_RGB_quick.tif\",\n",
    "                img_p + \"lap_2018-07_site04_120m_RGB_cc.tif\",\n",
    "                img_p + \"lap_2018-07_site06_120m_RGB_cc.tif\",\n",
    "                img_p + \"psc_2018-05_site01_120m_RGB_cc.tif\",\n",
    "                img_p + \"psc_2018-05_site11_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-05_site12_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-05_site8.tif\",\n",
    "                img_p + \"psc_2018-07_site08_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-07_site11_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-07_site10_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-07_site09_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-05_site13-14_120m_RGB.tif\",\n",
    "               ]\n",
    "\n",
    "vector_files = [\n",
    "                vec_p + \"lap_2018-07_site01_labels_m-nm.shp\", \n",
    "                vec_p + \"lap_2019-07_site03_labels_m-nm.shp\", \n",
    "                vec_p + \"lap_2018-07_site05_120m_m-nm_dissolve.shp\",\n",
    "                vec_p + \"lap_2019-07_site06_120m_labels_m-nm.shp\",\n",
    "                vec_p + \"lap_2018-07_site04_labels_m-nm.shp\",\n",
    "                vec_p + \"lap_2018-07_site06_120m_RGB_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-05_site01_120m_RGB_cc labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-05_site11_120m_RGB_dissolved.shp\",\n",
    "                vec_p + \"psc_2018-05_site12_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-05_site8_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-07_site08_120m_RGB_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-07_site11_120m_RGB_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-07_site_10_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_201807_site9_mnm.shp\",\n",
    "                vec_p + \"psc_2018_05_site1314_120m_mnm.shp\",\n",
    "               ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_setup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7234e0392685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_setup' is not defined"
     ]
    }
   ],
   "source": [
    "#Run Testing\n",
    "test_setup(raster_files, out_width)\n",
    "test(backbone, weight_file, vector_files, raster_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow ver. 2.2.0\n",
      "4 Physical GPUs, 4 Logical GPUs\n",
      "The Training Dataset contains 106560 images.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n",
      "Epoch 230/350\n",
      "INFO:tensorflow:batch_all_reduce: 38 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 38 all-reduces with algorithm = nccl, num_packs = 1\n",
      "   2/5328 [..............................] - ETA: 44:11 - loss: 0.2619 - iou_score: 0.7279WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.382765). Check your callbacks.\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1485 - iou_score: 0.8474\n",
      "Epoch 00230: val_loss improved from inf to 0.09223, saving model to ../dataset/training/weights/08_12_vgg16_350_full_weight.h5\n",
      "5328/5328 [==============================] - 1157s 217ms/step - loss: 0.1485 - iou_score: 0.8474 - val_loss: 0.0922 - val_iou_score: 0.9029\n",
      "Epoch 231/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1481 - iou_score: 0.8479\n",
      "Epoch 00231: val_loss improved from 0.09223 to 0.08685, saving model to ../dataset/training/weights/08_12_vgg16_350_full_weight.h5\n",
      "5328/5328 [==============================] - 1153s 216ms/step - loss: 0.1481 - iou_score: 0.8479 - val_loss: 0.0868 - val_iou_score: 0.9079\n",
      "Epoch 232/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1463 - iou_score: 0.8498\n",
      "Epoch 00232: val_loss did not improve from 0.08685\n",
      "5328/5328 [==============================] - 1152s 216ms/step - loss: 0.1463 - iou_score: 0.8498 - val_loss: 0.1186 - val_iou_score: 0.8763\n",
      "Epoch 233/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1452 - iou_score: 0.8508\n",
      "Epoch 00233: val_loss improved from 0.08685 to 0.08247, saving model to ../dataset/training/weights/08_12_vgg16_350_full_weight.h5\n",
      "5328/5328 [==============================] - 1151s 216ms/step - loss: 0.1452 - iou_score: 0.8508 - val_loss: 0.0825 - val_iou_score: 0.9124\n",
      "Epoch 234/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1480 - iou_score: 0.8481\n",
      "Epoch 00234: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1153s 216ms/step - loss: 0.1480 - iou_score: 0.8481 - val_loss: 0.1196 - val_iou_score: 0.8741\n",
      "Epoch 235/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1494 - iou_score: 0.8466\n",
      "Epoch 00235: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1153s 216ms/step - loss: 0.1494 - iou_score: 0.8466 - val_loss: 0.1112 - val_iou_score: 0.8832\n",
      "Epoch 236/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1459 - iou_score: 0.8501\n",
      "Epoch 00236: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1151s 216ms/step - loss: 0.1459 - iou_score: 0.8501 - val_loss: 0.1231 - val_iou_score: 0.8714\n",
      "Epoch 237/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1442 - iou_score: 0.8517\n",
      "Epoch 00237: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1153s 216ms/step - loss: 0.1442 - iou_score: 0.8517 - val_loss: 0.1046 - val_iou_score: 0.8899\n",
      "Epoch 238/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1463 - iou_score: 0.8496\n",
      "Epoch 00238: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1151s 216ms/step - loss: 0.1463 - iou_score: 0.8496 - val_loss: 0.1018 - val_iou_score: 0.8928\n",
      "Epoch 239/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1438 - iou_score: 0.8524\n",
      "Epoch 00239: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1152s 216ms/step - loss: 0.1438 - iou_score: 0.8524 - val_loss: 0.1094 - val_iou_score: 0.8857\n",
      "Epoch 240/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1474 - iou_score: 0.8484\n",
      "Epoch 00240: val_loss did not improve from 0.08247\n",
      "5328/5328 [==============================] - 1154s 217ms/step - loss: 0.1474 - iou_score: 0.8484 - val_loss: 0.1092 - val_iou_score: 0.8851\n",
      "Epoch 241/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1472 - iou_score: 0.8488\n",
      "Epoch 00241: val_loss improved from 0.08247 to 0.07981, saving model to ../dataset/training/weights/08_12_vgg16_350_full_weight.h5\n",
      "5328/5328 [==============================] - 1153s 216ms/step - loss: 0.1472 - iou_score: 0.8488 - val_loss: 0.0798 - val_iou_score: 0.9155\n",
      "Epoch 242/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1446 - iou_score: 0.8515\n",
      "Epoch 00242: val_loss improved from 0.07981 to 0.07214, saving model to ../dataset/training/weights/08_12_vgg16_350_full_weight.h5\n",
      "5328/5328 [==============================] - 1154s 217ms/step - loss: 0.1446 - iou_score: 0.8515 - val_loss: 0.0721 - val_iou_score: 0.9232\n",
      "Epoch 243/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1473 - iou_score: 0.8488\n",
      "Epoch 00243: val_loss did not improve from 0.07214\n",
      "5328/5328 [==============================] - 1154s 217ms/step - loss: 0.1473 - iou_score: 0.8488 - val_loss: 0.0755 - val_iou_score: 0.9202\n",
      "Epoch 244/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1482 - iou_score: 0.8480\n",
      "Epoch 00244: val_loss did not improve from 0.07214\n",
      "5328/5328 [==============================] - 1151s 216ms/step - loss: 0.1482 - iou_score: 0.8480 - val_loss: 0.0895 - val_iou_score: 0.9053\n",
      "Epoch 245/350\n",
      "5328/5328 [==============================] - ETA: 0s - loss: 0.1464 - iou_score: 0.8496\n",
      "Epoch 00245: val_loss did not improve from 0.07214\n",
      "5328/5328 [==============================] - 1152s 216ms/step - loss: 0.1464 - iou_score: 0.8496 - val_loss: 0.0974 - val_iou_score: 0.8979\n",
      "Epoch 246/350\n",
      "  30/5328 [..............................] - ETA: 17:57 - loss: 0.1277 - iou_score: 0.8696"
     ]
    }
   ],
   "source": [
    "#Run Training\n",
    "#train_setup(raster_files, vector_files, out_width)\n",
    "train(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 124870), started 0:00:45 ago. (Use '!kill 124870' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bac92200f84aba7b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bac92200f84aba7b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PATH = os.getenv('PATH')\n",
    "# %env PATH=/anaconda3/envs/py37_tensorflow/bin:$PATH\n",
    "# Does not work, need to do ssh forwarding with port on vm\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     TRAIN = False\n",
    "#     TEST = False\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"UNet Training and Inference Script (Note: order of rasters and vectors must correspond to one another)\")\n",
    "#     parser.add_argument(\"--width\",help = \"Width of output tiles\")\n",
    "#     parser.add_argument(\"--input_rasters\", nargs='*', help = \"space separated input orthomosaic (.tif)\")\n",
    "#     parser.add_argument(\"--input_vectors\", nargs='*', help = \"space separated input labels (.shp)\")\n",
    "#     parser.add_argument(\"--train\", action='store_true', help = \"training UNet\")\n",
    "#     parser.add_argument(\"--test\", action='store_true', help = \"testing UNet\")\n",
    "#     parser.add_argument(\"--weights\", help = \"path to weight file, either to save or use (.h5)\")\n",
    "#     parser.add_argument(\"--backbone\", help = \"segmentation model backbone, ex: resnet34, vgg16, etc.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Parsing arguments\n",
    "#     if args.width:\n",
    "#         out_width = args.width\n",
    "#     else:\n",
    "#         print(\"Need to specify width, exiting.\")\n",
    "#         exit()\n",
    "#     if args.input_rasters:\n",
    "#         raster_files = args.input_rasters\n",
    "#     else:\n",
    "#         # Always needs a raster\n",
    "#         print(\"Need to specify raster file, exiting.\")\n",
    "#         exit()\n",
    "#     if args.input_vectors:\n",
    "#         vector_files = args.input_vectors\n",
    "#     else:\n",
    "#         # Requires vector labes for training, not inference\n",
    "#         if args.train:\n",
    "#             print(\"Need to specify input vector, exiting.\")\n",
    "#             exit()\n",
    "#     if args.train and args.test:\n",
    "#         print(\"Can't train and test at the same time... exiting.\")\n",
    "#         exit()\n",
    "#     elif args.train:\n",
    "#         TRAIN = True\n",
    "#     elif args.test:\n",
    "#         TEST = True\n",
    "#     if args.weights:\n",
    "#         weight_file = args.weights\n",
    "#     else:\n",
    "#         print(\"Need weight file, exiting.\")\n",
    "#         exit()\n",
    "#     if args.backbone:\n",
    "#         backbone = args.backbone\n",
    "#     else:\n",
    "#         print(\"Need to specify backbone, exiting.\")\n",
    "#         exit()\n",
    "\n",
    "#     # Selecting mode\n",
    "#     if TRAIN: \n",
    "#         train_setup(raster_files, vector_files, out_width)\n",
    "#         train(backbone, weight_file)\n",
    "#     if TEST:\n",
    "#         test_setup(raster_files, out_width)\n",
    "#         test(backbone, weight_file)\n",
    "#         #test_optimized(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/training/vectors/masks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7e800fe4c6d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# shutil.rmtree(\"../dataset/testing/output\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# shutil.rmtree(\"../dataset/testing/images\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"vectors/masks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"vectors/nm\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#removng directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"vectors/m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/training/vectors/masks'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "#delete all training/images, /images/images, /images/labels, images/masks, \n",
    "#images/m, images/nm, annotations\n",
    "\n",
    "#delete all files with extension\n",
    "folderpath = \"../dataset/training/\"\n",
    "for file_name in os.listdir(folderpath + \"images/images\"):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        os.remove(folderpath + \"images/images/\" + file_name)\n",
    "for file_name in os.listdir(folderpath + \"images\"):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        os.remove(folderpath + \"images/\" + file_name)\n",
    "for file_name in os.listdir(folderpath + \"images/labels\"):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        os.remove(folderpath + \"images/labels/\" + file_name)\n",
    "# shutil.rmtree(\"../dataset/testing/output\")\n",
    "# shutil.rmtree(\"../dataset/testing/images\")\n",
    "shutil.rmtree(folderpath + \"vectors/masks\")\n",
    "shutil.rmtree(folderpath + \"vectors/nm\")  #removng directories\n",
    "shutil.rmtree(folderpath + \"vectors/m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
